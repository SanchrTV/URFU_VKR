## Основные причины применения Machine Unlearning
Machine Unlearning предоставляет механизм для удаления влияния отдельных данных из обученных моделей без необходимости полного переобучения. Этот процесс становится особенно актуальным в контексте растущих требований к обработке данных, обеспечения безопасности и адаптации моделей к изменениям в окружающей среде. Современные методы машинного разучивания развиваются в ответ на несколько ключевых факторов, определяющих необходимость избирательного удаления информации.
Одним из важнейших стимулов для разработки методов разучивания является рост числа нормативных актов, регулирующих обработку данных. Введение таких законов, как GDPR и CCPA, закрепило за пользователями право требовать удаления их персональной информации из автоматизированных систем обработки. В отличие от полного переобучения модели, которое является ресурсозатратным и зачастую неприменимым на практике, разучивание позволяет целенаправленно исключать определённые данные, обеспечивая их эффективное стирание и минимизируя вычислительные затраты [1.	Machine Unlearning via Algorithmic Stability / E. Ullah, T. Mai, A. Rao,  [et al.] arXiv:2102.13179 [cs]. – arXiv, 2021. – URL: http://arxiv.org/abs/2102.13179 (date accessed: 13.03.2025). – Text : electronic. ,, L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, "Machine unlearning," *IEEE Symposium on Security and Privacy (SP)*, 2021.]. Однако, несмотря на кажущуюся простоту, удаление данных из обученной модели сопряжено с серьёзными техническими трудностями, включая остаточное влияние удаляемых данных и возможность их частичного восстановления через косвенные связи в параметрах модели.
Помимо требований законодательства, машинное разучивание играет важную роль в защите моделей от внешних атак и нежелательных изменений. Системы машинного обучения подвержены различным видам атак, включая внедрение вредоносных данных с целью изменения предсказаний модели (data poisoning). В таких условиях методы разучивания позволяют изолировать и удалять данные, влияющие на устойчивость системы, предотвращая негативные последствия отравления обучающего набора [Liu, Y., Fan, M., Chen, C., et al. *Backdoor defense with machine unlearning*. IEEE INFOCOM, 2022., Wu, G., Hashemi, M., Srinivasa, C. *PUMA: Performance unchanged model augmentation for training data removal*. AAAI, 2022. ,, Y. Wu, E. Dobriban, and S. Davidson, "DeltaGrad: Rapid retraining of machine learning models," *International Conference on Machine Learning (ICML)*, 2020.]. Этот аспект особенно важен в критически значимых приложениях, где модели принимают решения в области медицины, финансов или автоматического управления, поскольку неправильная интерпретация данных может привести к серьёзным последствиям. Более того, разучивание используется не только для устранения атак, но и для исправления внутренних ошибок модели, включая предвзятость в предсказаниях, а также удаления потенциально нежелательной информации [Rethinking Machine Unlearning, 2024].
Другой важной причиной применения машинного разучивания является необходимость адаптации моделей к изменяющимся данным. В ряде случаев обучающий набор может устаревать или терять актуальность, что приводит к снижению точности предсказаний и появлению ошибок. Традиционные методы обновления моделей часто требуют полного переобучения, что не всегда возможно, особенно для сложных систем. Разучивание позволяет оперативно исключать устаревшие или нерелевантные данные, сохраняя при этом общую структуру модели и обеспечивая её адаптацию к новым условиям [Ma, Z., Liu, Y., Liu, X., et al. *Learn to forget: Machine unlearning via neuron masking*. IEEE Transactions on Dependable and Secure Computing, 2022., Yang, E., Shen, L., Wang, Z., et al. *An efficient dataset condensation plugin and its application to continual learning*. NeurIPS, 2023. ,, Y. Cao, J. Yang, *Towards Making Systems Forget with Machine Unlearning*, IEEE Symposium on Security and Privacy, 2015., L. Bourtoule, V. Chandrasekaran, C. Choquette-Choo et al., *Machine Unlearning*, IEEE Security and Privacy (SP), 2021.]. Это делает данный подход полезным в динамичных средах, где информация быстро устаревает, например, в системах прогнозирования или автоматического анализа данных.
Несмотря на очевидные преимущества, реализация методов разучивания сталкивается с рядом вызовов, включая точность удаления данных, вычислительные затраты и необходимость сохранения работоспособности модели после разучивания. Основная сложность заключается в том, что знания в обученной модели не локализованы в отдельных параметрах, а распределены по всей структуре, что затрудняет их точечное удаление. Кроме того, процесс удаления информации должен быть устойчивым к повторному восстановлению данных, что особенно важно при выполнении требований регуляторов. Вычислительная сложность методов разучивания также остаётся ключевым фактором, ограничивающим их широкое применение, поскольку многие современные алгоритмы требуют значительных ресурсов и времени на корректировку модели.
Для решения этих задач были предложены различные подходы к машинному разучиванию, отличающиеся степенью точности удаления данных и необходимыми вычислительными затратами. В зависимости от требований к эффективности и гарантии стирания информации, методы разучивания могут базироваться на различных принципах, включая полное переобучение, адаптивную модификацию параметров или приближённое устранение влияния данных. Рассмотрим основные методики, используемые для реализации машинного разучивания, их преимущества и ограничения.

## Классические подходы к Machine Unlearning
Классические методы машинного разучивания можно разделить на две основные категории: **точное разучивание (Exact Unlearning)** и **приближенное разучивание (Approximate Unlearning)**. Каждый из этих подходов имеет свои особенности, преимущества и ограничения в зависимости от сложности модели и требований к эффективности удаления влияния данных.

### Точное разучивание (Exact Unlearning)
Подходы точного разучивания направлены на полное устранение влияния удаляемых данных так, чтобы модель после разучивания вела себя так, как если бы эти данные никогда не использовались при обучении. Среди методов точного разучивания можно выделить:
- **SISA (Sharded, Isolated, Sliced, and Aggregated Training)**:  Этот алгоритм разбивает обучающие данные на изолированные сегменты, что позволяет локально удалять выбранные данные без необходимости полного переобучения всей модели. Такой подход минимизирует вычислительные затраты, обеспечивая точное разучивание только затронутых сегментов [L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, "Machine unlearning," *IEEE Symposium on Security and Privacy (SP)*, 2021.].
- **DeltaGrad**: Метод, основанный на концепции обратного обучения (*Reverse Learning*), при котором удаляемые данные используются для увеличения ошибки модели. В результате влияние этих данных нивелируется, и модель приближается к состоянию, как если бы данные никогда не присутствовали в обучающем наборе [Y. Wu, E. Dobriban, and S. Davidson, "DeltaGrad: Rapid retraining of machine learning models," *International Conference on Machine Learning (ICML)*, 2020.].

### Приближенное разучивание (Approximate Unlearning)
Приближенное разучивание ориентировано на снижение влияния определённых данных без гарантии полного их устранения. Такой подход часто выбирается для сложных моделей, например, глубоких нейросетей, где полное разучивание может быть вычислительно слишком затратным.
- **Сертифицированное удаление данных (Certified Data Removal)**: Этот метод использует модифицированные градиентные обновления и стохастические возмущения, чтобы уменьшить влияние удаляемых данных. Несмотря на то, что влияние не устраняется полностью, достигается приемлемый баланс между эффективностью разучивания и сохранением производительности модели [C. Guo, T. Goldstein, A. Hannun, and L. van der Maaten, "Certified data removal from machine learning models," *International Conference on Machine Learning (ICML)*, 2020.].
- **Лагранжева оптимизация**: Дополнительный подход, предложенный Golatkar et al., основанный на оптимизации с использованием лагранжевых множителей для избирательного удаления влияния определённых параметров модели. Этот метод позволяет адаптивно корректировать веса, минимизируя нежелательное влияние удаляемых данных, что особенно полезно при работе с большими моделями [A. Golatkar, A. Achille, and S. Soatto, "Eternal sunshine of the spotless net: Selective forgetting in deep networks," *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.].

### Основные вызовы классического Machine Unlearning
Несмотря на значительный прогресс в разработке методов машинного разучивания, существует ряд фундаментальных вызовов, которые необходимо решить для успешного применения этих технологий в реальных системах. Эти вызовы не только отражают сложности, связанные с текущими архитектурами и алгоритмами, но и предвещают направления для дальнейших исследований и разработки новых подходов.
Современные модели машинного обучения учатся на сложных корреляциях между обучающими примерами. При удалении отдельных записей или сегментов данных существует риск нарушения выявленных закономерностей, что может привести к снижению точности и ухудшению общего качества модели. Эффективное разучивание требует точного понимания взаимосвязей внутри обучающего набора, чтобы гарантировать, что удаление информации не повлияет негативно на производительность модели [A. Ginart, M. Guan, G. Valiant, and J. Y. Zou, "Making AI forget you: Data deletion in machine learning," *Advances in Neural Information Processing Systems (NeurIPS)*, 2019.].
Глубокие нейросети и другие современные архитектуры содержат миллионы параметров, что существенно усложняет задачу локализации и удаления влияния конкретных данных. Даже при использовании методов точного разучивания, таких как SISA или DeltaGrad, вызов заключается в том, чтобы изолировать вклад каждого обучающего примера и обеспечить его полное исключение без нарушения структуры модели. Эта проблема особенно актуальна для масштабных систем, где даже небольшие изменения могут иметь далеко идущие последствия [R. Mehta, S. Pal, V. Singh, and S. N. Ravi, "Deep unlearning via randomized conditionally independent Hessians," *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.].
Многие подходы к машинному разучиванию, особенно точные методы, требуют значительных вычислительных ресурсов. Повторное обучение части модели или корректировка весов с целью удаления влияния конкретных данных могут быть крайне затратными с точки зрения времени и вычислительной мощности. Это делает применение данных методов ограниченным для крупных или ресурсоемких систем, где оптимизация процесса разучивания становится критически важной при работе с моделями, содержащих миллиарды параметров [Z. Ma, Y. Liu, X. Liu, J. Liu, J. Ma, and K. Ren, "Learn to forget: Machine unlearning via neuron masking," *IEEE Transactions on Dependable and Secure Computing*, 2022.].

## Современные методы разучивания и их применения к большим языковым моделям
Большие языковые модели (LLMs) стали краеугольным камнем современных систем обработки естественного языка (NLP), демонстрируя выдающиеся результаты в генерации текста, машинном переводе, анализе тональности и других прикладных задачах [Zhao et al.,, 2023, OpenAI, 2023]. Их архитектура на основе трансформеров позволила значительно повысить качество обработки естественного языка, однако по мере роста масштабов моделей возрастает и ряд вызовов, связанных с их конфиденциальностью, адаптацией и безопасностью.
Одним из ключевых аспектов работы с LLMs является возможность целенаправленного удаления информации без полного переобучения модели. Это необходимо как с точки зрения соблюдения нормативных стандартов (например, **GDPR**, требующего механизма «права быть забытым»), так и для устранения предвзятости, обновления знаний и защиты от атак на данные [Nguyen et al., 2022,, Yao et al., 2023]. Однако классические методы машинного разучивания (Machine Unlearning, MU), такие как точное разучивание (Exact Unlearning) и приближенное разучивание (Approximate Unlearning), плохо применимы к большим языковым моделям из-за их масштабности, сложности параметрической структуры и высокой вычислительной стоимости. Чтобы понять, почему разучивание в LLMs требует новых подходов, рассмотрим ключевые архитектурные особенности этих моделей и их влияние на процесс удаления знаний.

## Архитектура и эволюция LLMs
Основой современных больших языковых моделей (LLMs) является архитектура **Transformer**, предложенная Васвани и соавторами в 2017 году, которая радикально изменила подходы к обработке естественного языка [Vaswani, A., et al. Attention Is All You Need. *NeurIPS*, 2017. DOI: [10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762).]. В центре Transformer лежит механизм self-attention, позволяющий оценивать важность каждого токена во входной последовательности, а также multihead attention, которое параллельно обрабатывает различные аспекты взаимосвязей между словами. Дополнительное позиционное кодирование обеспечивает сохранение информации о последовательности токенов, что критично для понимания синтаксиса и семантики текста.  
Эволюция LLMs продемонстрировала, как базовая архитектура может адаптироваться для решения различных задач. Так, в 2018 году **Google** представил модель **BERT**, которая использует двунаправленное кодирование контекста, позволяющее учитывать информацию с обеих сторон каждого слова и, таким образом, глубже понимать языковые нюансы [**Devlin, J., et al.** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL-HLT*, 2019. DOI: [10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423).].  
В свою очередь, модели семейства **GPT (Generative Pre-trained Transformer)**, разработанные **OpenAI**, ориентированы на авторегрессионную генерацию текста, где модель предсказывает следующий токен, основываясь на предыдущей последовательности. Примером является **GPT-3**, обладающая **175 миллиардами параметров**, что позволяет ей демонстрировать впечатляющие результаты в генерации текста [**Brown, T., et al.** Language Models Are Few-Shot Learners. *NeurIPS*, 2020. DOI: [10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165).].  
Модель **T5 (Text-to-Text Transfer Transformer)** представляет собой универсальную текстовую модель, которая формулирует все NLP-задачи как задачи преобразования текста, что значительно облегчает перенос обучения между различными задачами [**Raffel, C., et al.** Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*, 2020. DOI: [10.48550/arXiv.1910.10683](https://doi.org/10.48550/arXiv.1910.10683).].  
Наконец, новейшая модель **LLaMA 3**, выпущенная в апреле 2024 года, демонстрирует значительный скачок в масштабируемости и мультиязычной поддержке – она обучена на значительно большем объёме данных, содержит до **405 миллиардов параметров** и поддерживает **контекстное окно до 128 000 токенов**, что существенно превосходит предыдущие версии [**Touvron, H., et al.** LLaMA: Open and Efficient Foundation Language Models. *CoRR*, 2023. DOI: [10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971).].
Параллельно с разработкой новых архитектур активно совершенствуются методы **оптимизации и адаптации моделей**.  
- **Техники настройки на инструкциях (instruction tuning)** позволяют LLMs лучше справляться с разнообразными задачами за счёт дополнительного дообучения на специально подготовленных запросах [**Wei, J., et al.** Finetuned Language Models Are Zero-Shot Learners. *CoRR*, 2021. DOI: [10.48550/arXiv.2109.01652](https://doi.org/10.48550/arXiv.2109.01652).].  
- **Обучение с подкреплением (RLHF)** помогает моделям согласовываться с человеческими предпочтениями, повышая безопасность и качество генерируемых ответов, а также выучивает более сложные зависимости в данных [**Ouyang, L., et al.** Training Language Models to Follow Instructions with Human Feedback. *NeurIPS*, 2022. DOI: [10.48550/arXiv.2203.02155](https://doi.org/10.48550/arXiv.2203.02155).].  
- **Методы эффективного дообучения параметров (PEFT)**, такие как **LoRA** и **адаптеры**, позволяют существенно сократить вычислительные затраты при обновлении модели без полного переобучения [**Hu, E. J., et al.** LoRA: Low-Rank Adaptation of Large Language Models. *ICLR*, 2022.].  
Эволюция LLMs демонстрирует не только рост вычислительной мощности и масштабов моделей, но и совершенствование подходов к их оптимизации, что создаёт прочную основу для последующих исследований в области борьбы с ограничениями, вызовами в области предвзятости, галлюцинаций и методов машинного разучивания. Эти достижения задают вектор дальнейшего развития, где **интеграция методов динамического обновления и корректировки знаний** станет ключевым направлением для создания более **безопасных, адаптивных и этически обоснованных систем** [**Zhang, Y., et al.** Editing Large Language Models: Problems, Methods, and Opportunities. *EMNLP*, 2023., **Cao, B., et al.** Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. *CoRR*, 2023. DOI: [10.48550/arXiv.2305.09144](https://doi.org/10.48550/arXiv.2305.09144).].

## Ограничения и вызовы в LLMs
Несмотря на значительные достижения в архитектуре больших языковых моделей (LLMs), их масштаб и структура создают серьезные вызовы при удалении информации. Эти ограничения связаны не только с техническими аспектами, но и с вопросами социальной ответственности, что становится особенно актуальным на фоне стремительного роста возможностей моделей и их применения в чувствительных областях, таких как медицина, право и безопасность данных [**Zhang, Y., et al.** Editing Large Language Models: Problems, Methods, and Opportunities. *EMNLP*, 2023., **Cao, B., et al.** Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. *CoRR*, 2023. DOI: [10.48550/arXiv.2305.09144](https://doi.org/10.48550/arXiv.2305.09144). ; Yejin Choi. "Knowledge is Power: Symbolic Knowledge Distillation, Commonsense Morality, & Multimodal Script Knowledge." *WSDM 22: The Fifteenth ACM International Conference on Web Search and Data Mining*, 2022. DOI: [10.1145/3488560.3500242](https://doi.org/10.1145/3488560.3500242).].  
Большие языковые модели создают три фундаментальные проблемы для машинного разучивания (Machine Unlearning, MU). Во-первых, традиционные методы, такие как *Sharded, Isolated, Sliced, and Aggregated Training (SISA)* [Bourtoule et al., 2021] или *Certified Data Removal* [Guo et al., 2020], были разработаны для небольших моделей и требуют переобучения значительных частей сети. В LLMs, содержащих сотни миллиардов параметров, такой процесс становится непрактичным. Во-вторых, архитектура трансформеров использует механизмы самообучения и многоголового внимания [Vaswani et al., 2017], что приводит к тому, что знания распределяются по всей сети. Это делает удаление отдельных данных сложным, так как модель сохраняет их в скрытых представлениях даже после разучивания. В-третьих, даже после удаления информации LLMs могут её восстанавливать через косвенные связи, особенно при дальнейшем дообучении. Это затрудняет соблюдение нормативных требований, таких как GDPR, требующий безвозвратного удаления персональных данных [Cao et al., 2015]. В результате традиционные методы MU плохо применимы к LLMs, поскольку либо неэффективны, либо не гарантируют полного удаления данных.  
Большие языковые модели обучаются с помощью *self-supervised learning*, что приводит к распределению знаний по всему набору параметров. В отличие от традиционных моделей, в которых знания локализованы в отдельных нейронах или слоях [Geva et al., 2021], в LLMs информация о конкретном факте может быть зашифрована в различных частях сети. Это создаёт значительные сложности при попытке целевого разучивания. Исследования показали, что LLMs сохраняют информацию даже после удаления данных из обучающего набора [Shumailov et al., 2023]. Например, даже если удалить конкретное имя из данных обучения, модель может продолжать его воспроизводить, опираясь на контекстные связи. Это критично для задач защиты конфиденциальности и регулирования обработки персональных данных.  
Даже после удаления части данных из модели их влияние может сохраняться в латентных представлениях. Это явление, известное как *residual memorization* [Tirumala et al., 2022], делает разучивание неэффективным: удалённые факты могут быть восстановлены путём дообучения на новых данных или через переобучение модели на схожих примерах. Таким образом, глубокая ретентивность знаний требует разработки новых методов MU, способных работать с высокосвязанными параметрическими пространствами.  
Одним из ключевых вызовов MU в LLMs является катастрофическое забывание (*catastrophic forgetting*). При удалении одного знания могут пострадать другие, связанные с ним концепции, что приводит к общей деградации модели. Например, если удалить все примеры, связанные с одним историческим событием, это может повлиять на генерацию текстов по смежным темам. В сложных моделях, таких как T5, информация представлена не в явном виде, а через множество параметров и статистических закономерностей [Brown et al., 2020]. Это делает MU нестабильным: удаление одной информации может случайно повлиять на другую.  
Не все знания модели одинаково важны для её работы. Исследования показывают, что удаление часто встречающихся данных из обучающего набора оказывает минимальное влияние, тогда как редкие примеры являются ключевыми для генерации высококачественных ответов [Golatkar et al., 2020]. Методы MU должны учитывать, какие знания можно удалить без ущерба для общей работоспособности модели. Однако большинство существующих алгоритмов разучивания не имеют механизмов автоматического определения таких "критических" знаний, что делает их применение в LLMs проблематичным.  
Модели типа GPT-4 или PaLM содержат сотни миллиардов параметров, а их полное переобучение занимает недели даже на суперкомпьютерах [Chowdhery et al., 2022]. Любой метод разучивания, требующий значительного пересчёта весов модели, становится непрактичным. Традиционные методы MU, такие как *SISA* или *Certified Data Removal*, были разработаны для относительно малых моделей [Bourtoule et al., 2021]. В LLMs они требуют пропорционально больше вычислительных ресурсов, что делает их неэффективными.  
Одним из неожиданных эффектов LLMs является их способность "вспоминать" ранее удалённую информацию. Даже после применения MU модель может восстанавливать знания при дообучении на схожих данных или при взаимодействии с пользователями [Carlini et al., 2024]. Это представляет угрозу для регуляторных стандартов, таких как GDPR, требующих окончательного удаления данных. В случае, если модель продолжает генерировать удалённые знания, это может привести к юридическим последствиям, особенно в сфере защиты персональных данных.  
Таким образом, традиционные методы машинного разучивания были разработаны для небольших моделей и плохо применимы к LLMs из-за их высокой вычислительной сложности, глубокой ретентивности знаний и эффекта переучивания. Проблемы, связанные с катастрофическим забыванием, долговременным запоминанием и невозможностью точного редактирования параметров модели, делают классические MU-алгоритмы неэффективными для LLMs.

## Современные методы разучивания LLM
В связи с тем, что классические методы machine unlearning являются ресурсоёмкими и непрактичными для LLMs [Ginart et al., 2019, Brown et al., 2020], исследователи обратились к методам, направленным на *разучивание без переобучения* (*post hoc unlearning*), среди которых одним из наиболее изучаемых является метод градиентного подъема (*Gradient Ascent, GA*).
Суть метода градиентного подъема заключается в целенаправленном увеличении ошибки модели на данных, которые должны быть забыты. Формально, если при обучении модели минимизируется функция потерь \(\mathcal{L}(\theta)\) с помощью градиентного спуска, то при применении GA используется противоположное направление обновления параметров: \[ \theta = \theta + \eta \nabla_{\theta} \mathcal{L}(\theta) \], где \(\eta\) — коэффициент скорости обучения, а \(\nabla_{\theta} \mathcal{L}(\theta)\) — градиент функции потерь, вычисленный по забываемому множеству данных (*forget set*). Такая инверсия позволяет разрушить связь между моделью и забываемыми примерами, тем самым снижая вероятность их корректного предсказания [Tarun et al., 2022]. Однако, несмотря на концептуальную простоту, метод градиентный подъема сталкивается с рядом серьёзных ограничений при применении к LLMs.
Во-первых, GA приводит к резкому расхождению модели, нарушая её способность выполнять исходные задачи. Это происходит из-за того, что увеличение ошибки на забываемых данных может затронуть параметры, ответственные за генерацию корректных предсказаний в других областях. Вследствие этого, модель начинает выдавать хаотичные или бессмысленные выходы, что делает метод непрактичным для реального использования [Shumailov et al., 2023]. Во-вторых, модель, подвергнутая GA, может столкнуться с эффектом *катастрофического разрушения* (*catastrophic collapse*), когда её параметры изменяются настолько радикально, что восстановление её первоначальной функциональности становится невозможным [Golatkar et al., 2020]. Наконец, даже после многократного применения GA удалённые данные могут частично сохраняться в скрытых представлениях модели, что снижает эффективность метода для задач, требующих строгого соответствия нормативным требованиям, таким как *право быть забытым* в рамках GDPR [Bourtoule et al., 2021].
Negative Preference Optimization (NPO), предложенный Zhang и др. (2024), представляет собой более стабильную альтернативу GA, обеспечивающую контролируемое забывание информации в больших языковых моделях (LLMs). Основная концепция метода заключается в исключении нежелательных данных (*forget set*) таким образом, чтобы минимизировать вероятность их предсказания, но при этом не допустить резкого ухудшения качества модели. В отличие от GA, которое просто увеличивает ошибку модели на забываемых примерах, NPO вводит специальную функцию потерь, позволяющую контролировать скорость расхождения модели и минимизировать побочные эффекты разучивания. Функция потерь NPO определяется выражением \(\mathcal{L}_{\text{NPO},\beta}(\theta) = \frac{2}{\beta} \mathbb{E}_{\mathcal{D}_{\text{FG}}} \left[ \log \left( 1 + \left( \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)} \right)^{\beta} \right) \right]\), где \(\beta\) — параметр, регулирующий скорость расхождения модели, \(\pi_{\theta}(y|x)\) — вероятностное распределение выходов модели \(\theta\), \(\pi_{\text{ref}}(y|x)\) — распределение базовой (референсной) модели, а \(\mathcal{D}_{\text{FG}}\) — множество забываемых данных.
Преимущества метода NPO заключаются в его способности обеспечивать экспоненциально более медленное расхождение модели по сравнению с GA, что предотвращает резкие изменения в параметрах и сохраняет структуру модели. Кроме того, NPO позволяет гибко настраивать степень забывания, варьируя параметр \(\beta\), что делает его универсальным инструментом для балансировки между забыванием и сохранением полезных знаний. В сравнении с GA, NPO демонстрирует стабильность процесса разучивания, что критически важно для LLMs, в которых ошибка в забывании может привести к глобальному ухудшению качества предсказаний.
В эмпирических исследованиях на датасете TOFU метод NPO показал существенное преимущество перед методом градиентным подъемом. При забывании 50% данных модель, использующая NPO, сохраняла значительную часть своих исходных возможностей и демонстрировала лучшие результаты на *retain set*, что подтверждает её способность к избирательному разучиванию без разрушения полезных знаний. Теоретическое объяснение эффективности NPO заключается в его градиентной структуре, которая предотвращает резкие изменения параметров модели. Градиент функции потерь NPO имеет вид \(\nabla_{\theta} \mathcal{L}_{\text{NPO},\beta} = \mathbb{E}_{\mathcal{D}_{\text{FG}}} \left[ \mathsf{W}_{\theta}(x, y) \nabla_{\theta} \log \pi_{\theta}(y|x) \right]\), где весовая функция \(\mathsf{W}_{\theta}(x, y)\) регулирует влияние забываемых примеров на обучение, замедляя процесс расхождения, если вероятность предсказания модели \(\pi_{\theta}(y|x)\) становится намного меньше, чем у референсной модели \(\pi_{\text{ref}}(y|x)\).
Применение NPO актуально не только в контексте удаления персональных данных, но и для корректировки знаний модели в задачах по борьбе с предвзятостью, адаптации к изменяющимся условиям и улучшению безопасности LLMs. NPO не требует изменения весов отдельных параметров, а использует более общий механизм управления вероятностями предсказаний. Это делает его более гибким инструментом, пригодным для широкого спектра задач, связанных с машинным разучиванием.
Метод SimNPO представляет собой усовершенствованный вариант Negative Preference Optimization, в котором устранена зависимость от эталонной модели, а также оптимизирована функция потерь, которая улучшает забывание данных. В отличие от NPO, SimNPO устраняет зависимость от референсной модели, делая процесс забывания более управляемым и устойчивым к изменениям в данных. Одним из центральных элементов SimNPO является нормализация функции потерь по длине ответа, что позволяет динамически адаптировать градиентное влияние на удаляемые примеры. В традиционных методах градиентного асценда удаление длинных текстов требует больших оптимизационных шагов, что приводит к их более медленному забыванию. SimNPO решает эту проблему, вводя нормализованную функцию награды: \ell_{\text{SimNPO}}(\theta) = \mathbb{E}{(x, y) \in \mathcal{D}{\text{f}}} \left[ -\frac{2}{\beta} \log \sigma \left( -\frac{\beta}{|y|} \log \pi_{\theta}(y|x) \right) \right]
где:
\beta — температурный параметр, регулирующий интенсивность разучивания; 
|y| — длина ответа, используемая для нормализации; 
\sigma(t) — сигмоидная функция; 
\pi_{\theta}(y|x) — вероятность генерации ответа моделью \theta.
Благодаря этому SimNPO перераспределяет оптимизационные усилия, обеспечивая сбалансированное разучивание независимо от длины удаляемых примеров. Этот подход решает три ключевые проблемы NPO: 
(1) предотвращает чрезмерное удаление одних примеров и недостаточное удаление других, 
(2) делает процесс разучивания более устойчивым к атакам повторного обучения и 
(3) обеспечивает более эффективное масштабирование на больших объемах данных.
Экспериментальное сравнение SimNPO и NPO. Для оценки эффективности SimNPO проведены эксперименты на нескольких бенчмарках разучивания:
	•	TOFU — задача по удалению вымышленных авторов из текстов;
	•	MUSE — тестирование разучивания реальных фактов, извлеченных из книг и новостных статей;
	•	WMDP — устранение вредоносных знаний, связанных с биобезопасностью.
Результаты показали, что SimNPO достигает точности разучивания, сравнимой с полным переобучением модели (Retrain), но при этом значительно превосходит NPO по устойчивости к потерям полезных знаний. В частности, при удалении 50% данных SimNPO сохраняет на 15% больше информации на retain set по сравнению с NPO, что подтверждает его способность к избирательному разучиванию без разрушения общей производительности модели. Более того, анализ показал, что SimNPO более равномерно распределяет усилия по забыванию сложных и простых примеров, тогда как NPO склонен к чрезмерному разучиванию одних данных и недостаточному разучиванию других.
Дополнительным преимуществом SimNPO является его устойчивость к атакам повторного обучения (relearning attacks). В традиционных методах разучивания модель может восстанавливать забытые данные при последующем обучении на схожих примерах, особенно если в процессе fine-tuning используются данные, близкие к забытым [Shumailov et al., 2023]. В SimNPO этот эффект значительно ослабляется за счет нормализованного сглаживания градиента: удаляемые примеры равномерно распределяются по всему спектру вероятностей модели, что делает их восстановление менее вероятным.
Для проверки этой гипотезы была проведена серия тестов с частичным повторным обучением на данных, схожих с забываемыми. В результате модели, использующие GA и NPO, восстанавливали до 60% забытых данных после повторного fine-tuning, тогда как у SimNPO этот показатель не превышал 20%. Таким образом, SimNPO демонстрирует наибольшую устойчивость к атакам повторного обучения среди всех протестированных методов.
Таким образом, SimNPO предложил более гибкое и эффективное решение этих проблем за счет отказа от референсного распределения и введения нормализованной функции награды.
Преимущества SimNPO делают его перспективным инструментом для широкого круга задач. В частности, его можно использовать для:
	1.	Обеспечения соблюдения GDPR — точного удаления персональных данных без полного переобучения модели;
	2.	Коррекции вредоносных знаний — устранения дезинформации и предвзятых знаний без ухудшения полезной функциональности модели;
	3.	Адаптации LLMs к изменениям в данных — динамического обновления знаний без необходимости полной перестройки модели.
Несмотря на значительный прогресс в разработке методов разучивания для LLMs, таких как Gradient Ascent, Negative Preference Optimization (NPO) и SimNPO, эти подходы сталкиваются с фундаментальными ограничениями. Они ориентированы на минимизацию вероятности предсказания нежелательных данных, но при этом не учитывают контекстную значимость удаляемой информации. В реальных сценариях далеко не всегда требуется полное забывание — в ряде случаев более эффективным решением является не стирание знания, а его коррекция. Это особенно актуально для задач устранения предвзятости, обновления устаревшей информации и контроля за выводами модели.

## Интеграция редактирования знаний и разучивания
Направление Knowledge Editing представляет собой альтернативный подход к классическим методам Machine Unlearning, ориентируясь не на полное разучивание, а на целенаправленную модификацию знаний модели. Вместо удаления информации, что может привести к утрате связанных контекстов, редактирование позволяет избирательно корректировать содержимое модели, минимизируя влияние на её общее поведение.
Методы, представленные в работе "A Comprehensive Study of Knowledge Editing for Large Language Models", демонстрируют возможность эффективного внесения изменений в знания модели при минимальных вычислительных затратах и без существенного ухудшения качества генерации. Такой подход обеспечивает более точное управление содержимым модели, сохраняя её когерентность и адаптивность к изменяющимся требованиям.
Исследования показывают, что вместо глобального разучивания более эффективны методы локального редактирования знаний. Подход, предложенный авторами, позволяет изменять отдельные факты в модели, не затрагивая её основную архитектуру. Этот фреймворк предлагает модульный подход к внесению изменений в параметры моделей, минимизируя воздействие на нерелевантные знания. Среди ключевых методов, поддерживаемых фреймворком EasyEdit, можно выделить ROME, MEMIT, MEND, PMET, которые применяются к различным архитектурам, включая T5, GPT-J, LLaMA и другие. Основной целью библиотеки EasyEdit является обеспечение эффективного внесения точечных изменений в знания модели без необходимости её полного переобучения, что критически важно для приложений, где требуется оперативное обновление информации [A Comprehensive Study of Knowledge Editing for Large Language Models / N. Zhang, Y. Yao, B. Tian,  [et al.] arXiv:2401.01286 [cs]. – arXiv, 2024. – URL: http://arxiv.org/abs/2401.01286 (date accessed: 13.03.2025). – Text : electronic.].
Методы редактирования знаний можно разделить на три основных подхода. Первый из них включает методы, основанные на памяти. Они позволяют хранить изменённые знания в отдельных модулях, что обеспечивает гибкость и контроль за редактированием. В рамках этого подхода можно выделить Self-Reflective Auto-Encoder (SERAC) и In-Context Editing (IKE). Первый метод использует внешний классификатор для определения необходимости редактирования запроса, после чего в случае обнаружения несовпадения знаний SERAC извлекает альтернативные ответы из памяти. Такой метод обладает высокой точностью редактирования за счёт сохранения исходных знаний, однако требует дополнительной памяти для хранения информации, что увеличивает время отклика модели. В отличие от него, IKE использует знания в контексте запроса, не изменяя параметры модели, что позволяет ему адаптироваться к новым данным в реальном времени. Однако его эффективность зависит от качества вводимого контекста, что может затруднять генерацию корректных ответов в нестандартных ситуациях.
Другим подходом является использование мета-обучения для редактирования знаний. Такой метод позволяет моделям быстро адаптироваться к изменениям, минимизируя влияние на основную структуру. В рамках этого направления можно выделить MEND (Memory-Efficient Neural Editing), который использует сеть с низкоразмерными градиентами для минимизации влияния изменений на другие части модели. Это позволяет эффективно вносить коррективы в знания модели без необходимости её полного переобучения, при этом сохраняется общий контекст модели. Однако MEND требует предварительного обучения дополнительной сети и может быть менее точным при сложных изменениях. Другой метод, Knowledge Editor (KE), использует нейронную сеть для предсказания необходимых изменений весов модели. Это позволяет выполнять точечные корректировки с высокой скоростью, но ограничивает способность модели к обобщению новых знаний.
Наиболее перспективным направлением является методика «локализация-редактирование», позволяющая сначала определить параметры модели, содержащие изменяемые знания, а затем внести точечные изменения в эти параметры. Среди таких методов выделяется Rank-One Model Editing (ROME), который использует методы каузального анализа для выявления слоёв модели, ответственных за конкретные знания. Этот метод обеспечивает высокую точность редактирования при минимальном влиянии на остальную часть модели, однако требует сложного анализа её структуры и может быть трудным для масштабирования. Ещё одним важным методом является Mass-Editing Memory in Transformers (MEMIT), который позволяет вносить массовые изменения в знания модели через обновление нескольких слоёв. Этот метод особенно полезен, когда требуется внести большое количество правок сразу, однако возможны нежелательные побочные эффекты на связанную информацию. Наиболее точечным методом редактирования является Precise Model Editing in Transformer (PMET), который поддерживает точечное редактирование параметров модели с сохранением контекста. Это делает его отличным инструментом для редактирования отдельных фактов, однако его применение требует сложных вычислений и ограничено в редактировании сложных зависимостей между знаниями.
Эффективность разучивания и редактирования знаний оценивается по нескольким ключевым метрикам. Среди них reliability (надежность), которая отражает, насколько успешно информация была удалена из модели, locality (локальность), показывающая, насколько изменения затронули только целевые знания, не повлияв на остальные, generalization (обобщение), демонстрирующая, как разучивание повлияло на смежные знания, portability (переносимость), оценивающая способность модели адаптироваться к изменению знаний. Эти параметры позволяют объективно измерять эффективность разучивания и выбирать наиболее подходящие методы для конкретных приложений.
Эксперименты с EasyEdit на моделях LLaMA-2 (7B) показали, что SERAC и IKE достигли 99% успешности редактирования, MEMIT и PMET обеспечили максимальную локальность изменений, а MEND продемонстрировал высокий уровень надёжности без значительного ухудшения других параметров модели. Эти результаты подтверждают, что методы редактирования знаний в больших языковых моделях могут эффективно заменять традиционные подходы к разучиванию, обеспечивая более точечное и эффективное управление знаниями без необходимости полного переобучения модели. В перспективе развитие таких методов позволит улучшить адаптивность LLMs, делая их более гибкими в отношении обновления информации и управления знаниями в условиях быстро меняющихся требований пользователей.

## Заключение
Машинное разучивание (Machine Unlearning, MU) представляет собой ключевую область исследований в машинном обучении, направленную на избирательное удаление информации из обученных моделей без необходимости их полного переобучения. В современных условиях, когда требования к безопасности данных, адаптации моделей и коррекции знаний становятся критически важными, MU играет важную роль в обеспечении конфиденциальности, защите моделей от атак и актуализации знаний. 
В ходе рассмотренных методов были выделены две основные категории разучивания: **точное (Exact Unlearning)** и **приближённое (Approximate Unlearning)**. Точные методы, такие как **SISA (Sharded, Isolated, Sliced, and Aggregated Training)** и **DeltaGrad**, обеспечивают детерминированное удаление информации, однако требуют значительных вычислительных ресурсов. Приближённые методы, включая **сертифицированное удаление данных (Certified Data Removal)** и **лагранжеву оптимизацию**, ориентированы на снижение влияния удаляемых данных без строгих гарантий их исчезновения.
Современные методы MU адаптируются для больших языковых моделей (LLMs), таких как **GPT, BERT, T5, LLaMA** и др., однако архитектурные особенности трансформеров делают процесс удаления информации сложным. Знания в LLMs распределены по всей структуре модели, что приводит к эффекту **residual memorization** — даже после удаления данных информация может сохраняться в скрытых представлениях. Более того, традиционные методы MU сталкиваются с проблемой **катастрофического забывания**, когда удаление одного знания затрагивает смежные концепции, нарушая работоспособность модели.
В ответ на эти вызовы были предложены методы **Negative Preference Optimization (NPO)** и его усовершенствованный вариант **SimNPO**. Они позволяют целенаправленно изменять вероятностные распределения модели, контролируя степень забывания информации без разрушения полезных знаний. Экспериментальные результаты показывают, что SimNPO превосходит традиционные методы MU по устойчивости к атакам повторного обучения и эффективности удаления данных. 
Несмотря на успехи в разработке методов разучивания, важной альтернативой являются подходы к **редактированию знаний (Knowledge Editing)**, позволяющие модифицировать конкретные факты в модели без глобального разучивания. Одним из наиболее перспективных направлений является фреймворк **EasyEdit**, включающий методы **ROME, MEMIT, MEND и PMET**, которые позволяют вносить точечные изменения в параметры моделей без разрушения их общей структуры.
Для детального анализа эффективности MU и редактирования знаний в практической части исследования будет проведено сравнительное изучение двух подходов: **EasyEdit и SimPO (Simulated Preference Optimization)**. **EasyEdit** ориентирован на локальное редактирование информации, обеспечивая высокую точность и минимальное влияние на остальную модель, в то время как **SimPO** представляет собой метод разучивания, оптимизированный для работы с LLMs. Сравнительный анализ позволит определить оптимальные стратегии управления знаниями в больших языковых моделях, балансируя между эффективностью удаления и сохранением целостности модели.
