1. Abadi M. [и др.]. Deep Learning with Differential Privacy // 2016.

2. Shokri R. [и др.]. Membership Inference Attacks against Machine Learning Models // 2017.

3. Finn C., Abbeel P., Levine S. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks // 2017.

4. Vaswani A. [и др.]. Attention Is All You Need // 2023.

5. Frankle J., Carbin M. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks // 2019.

6. Wang A. [и др.]. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding // 2019.

7. Devlin J. [и др.]. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding // 2019.

8. Liu N. F. [и др.]. Linguistic Knowledge and Transferability of Contextual Representations // 2019.

9. Ginart A. [и др.]. Making AI Forget You: Data Deletion in Machine Learning // 2019.

10. Raffel C. [и др.]. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer // 2023.

11. Guo C. [и др.]. Certified Data Removal from Machine Learning Models // 2023.

12. Golatkar A., Achille A., Soatto S. Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks // 2020.

13. Bourtoule L. [и др.]. Machine Unlearning // 2020.

14. Baumhauer T., Schöttle P., Zeppelzauer M. Machine Unlearning: Linear Filtration for Logit-based Classifiers // 2020.

15. Liu Y. [и др.]. Learn to Forget: Machine Unlearning via Neuron Masking // 2021.

16. Khashabi D. [и др.]. UnifiedQA: Crossing Format Boundaries With a Single QA System // 2020.

17. Brown T. B. [и др.]. Language Models are Few-Shot Learners // 2020.

18. Wu Y., Dobriban E., Davidson S. B. DeltaGrad: Rapid retraining of machine learning models // 2020.

19. Schick T., Schütze H. It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners // 2021.

20. Ullah E. [и др.]. Machine Unlearning via Algorithmic Stability // 2021.

21. Sekhari A. [и др.]. Remember What You Want to Forget: Algorithms for Machine Unlearning // 2021.

22. Zhang H. [и др.]. ASER: Towards Large-scale Commonsense Knowledge Acquisition via Higher-order Selectional Preference over Eventualities // 2022.

23. Hu E. J. [и др.]. LoRA: Low-Rank Adaptation of Large Language Models // 2021.

24. Wei J. [и др.]. Finetuned Language Models Are Zero-Shot Learners // 2022.

25. Liu Y. [и др.]. Backdoor Defense with Machine Unlearning // 2022.

26. Meng K. [и др.]. Locating and Editing Factual Associations in GPT // 2023.

27. Ouyang L. [и др.]. Training language models to follow instructions with human feedback // 2022.

28. Mehta R. [и др.]. Deep Unlearning via Randomized Conditionally Independent Hessians // 2022.

29. Jagielski M. [и др.]. Measuring Forgetting of Memorized Training Examples // 2023.

30. Nguyen T. T. [и др.]. A Survey of Machine Unlearning // 2024.

31. Shi Y. [и др.]. Improving the Model Consistency of Decentralized Federated Learning // 2023.

32. Touvron H. [и др.]. LLaMA: Open and Efficient Foundation Language Models // 2023.

33. OpenAI [и др.]. GPT-4 Technical Report // 2024.

34. Shen Y. [и др.]. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face // 2023.

35. Zhao W. X. [и др.]. A Survey of Large Language Models // 2025.

36. Yang J. [и др.]. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond // 2023.

37. Gurnee W. [и др.]. Finding Neurons in a Haystack: Case Studies with Sparse Probing // 2023.

38. Qu Y. [и др.]. Learn to Unlearn: A Survey on Machine Unlearning // 2023.

39. Cao B. [и др.]. Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models // 2024.

40. Yao Y. [и др.]. Editing Large Language Models: Problems, Methods, and Opportunities // 2023.

41. Xu H. [и др.]. Machine Unlearning: A Survey // 2023.

42. Yin S. [и др.]. A Survey on Multimodal Large Language Models // 2024.

43. Naveed H. [и др.]. A Comprehensive Overview of Large Language Models // 2024.

44. Wang Z. [и др.]. A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning // 2024.

45. Xu J. [и др.]. Machine Unlearning: Solutions and Challenges // 2024.

46. Xu J. [и др.]. Machine Unlearning: Solutions and Challenges // 2024.

47. Wang P. [и др.]. EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models // 2024.

48. Hernandez E. [и др.]. Linearity of Relation Decoding in Transformer Language Models // 2024.

49. Gallegos I. O. [и др.]. Bias and Fairness in Large Language Models: A Survey // 2024.

50. Xi Z. [и др.]. The Rise and Potential of Large Language Model Based Agents: A Survey // 2023.

51. Zhang N. [и др.]. A Comprehensive Study of Knowledge Editing for Large Language Models // 2024.

52. Tonmoy S. M. T. I. [и др.]. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models // 2024.

53. Liu S. [и др.]. Rethinking Machine Unlearning for Large Language Models // 2024.

54. Yao J. [и др.]. Machine Unlearning of Pre-trained Large Language Models // 2024.

55. Xu Y. Machine Unlearning for Traditional Models and Large Language Models: A Short Survey // 2024.

56. Zhang R. [и др.]. Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning // 2024.

57. Zhang R. [и др.]. Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning // 2024.

58. Wang W. [и др.]. Machine Unlearning: A Comprehensive Survey // 2024.

59. Wang P. [и др.]. WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models // 2024.

60. Besta M. [и др.]. CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks // 2024.

61. Ji J. [и др.]. Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference // 2024.

62. Song J., Yu S., Yoon S. Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination // 2024.

63. Fan C. [и др.]. Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning // 2025.

64. Li D. [и др.]. CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models // 2024.

65. Cao Y., Yang J. Towards Making Systems Forget with Machine Unlearning San Jose, CA: IEEE, 2015.C. 463–480.

66. Du M. [и др.]. Lifelong Anomaly Detection Through Unlearning London United Kingdom: ACM, 2019.C. 1283–1297.

67. Hong Y. [и др.]. Dissecting Fine-Tuning Unlearning in Large Language Models Miami, Florida, USA: Association for Computational Linguistics, 2024.C. 3933–3941.

68. Ketkar N. Deep learning with Python: a hands-on introduction / N. Ketkar, New York, NY: Apress, 2017. 226 c.

69. Mantelero A. Corrigendum to “The EU Proposal for a General Data Protection Regulation and the roots of the ‘right to be forgotten’” [2013] 29 CLSR 229–235 // Computer Law & Security Review. 2013. № 5 (29). C. 637.

70. Mundt M., Pliushch I., Ramesh V. Neural Architecture Search of Deep Priors: Towards Continual Learning without Catastrophic Interference Nashville, TN, USA: IEEE, 2021.C. 3518–3527.

71. Neelakantan A., Roth B., McCallum A. Compositional Vector Space Models for Knowledge Base Completion Beijing, China: Association for Computational Linguistics, 2015.C. 156–166.

72. Wichert L., Sikdar S. Rethinking Evaluation Methods for Machine Unlearning Miami, Florida, USA: Association for Computational Linguistics, 2024.C. 4727–4739.

73. Wolf T. [и др.]. Transformers: State-of-the-Art Natural Language Processing Online: Association for Computational Linguistics, 2020.C. 38–45.

74. Zhang D. [и др.]. To be forgotten or to be fair: unveiling fairness implications of machine unlearning methods // AI and Ethics. 2024. № 1 (4). C. 83–93.

75. Advancing software engineering through AI, federated learning, and large language models под ред. A. K. Sharma [и др.]., Hershey, Pennsylvania (701 E. Chocolate Avenue, Hershey, Pennsylvania, 17033, USA): IGI Global, 2024. 1 c.
